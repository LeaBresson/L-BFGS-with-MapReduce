{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-scale L-BFGS using MapReduce\n",
    "\n",
    "*Eléments logiciels pour le traitement des données massives*\n",
    "\n",
    "Léa Bresson - Kolia Iakovlev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "\n",
    "[1. Introduction](#pb)<br>\n",
    "[2. The logistic function](#fct)<br>\n",
    "[3. Generate a dataset](#data)<br>\n",
    "[4. L-BFGS with scipy](#sc)<br>\n",
    "[5. L-BFGS with MapReduce](#algo1)<br>\n",
    "[6. VL-BFGS with MapReduce](#algo2)<br>\n",
    "[7. Conclusion](#conc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "sc = SparkContext.getOrCreate()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import norm\n",
    "import random as random\n",
    "import time\n",
    "# to embed figures in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pb'></a>\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Limited-memory BFGS** (L-BFGS) has been a very popular algorithm for parameter estimation in machine learning since 1980s. The L-BGFS is an iterative method for solving unconstrained nonlinear optimization problems; it belongs to quasi-Newton methods. For such problems, a necessary condition for optimality is that the gradient be zero. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "More precisely, Newton method is a method used to find the zeros of a differentiable non linear function $g$, x such that $g(x) = 0$, where $g : \\mathbb{R^n} \\rightarrow \\mathbb{R^n}$. Given a starting point $x_0$, Newton method consists in iterating: \n",
    "$$ x_{k+1} = x_k - g'(x_k)^{-1}g(x_k) $$ where $g'(x)$ is the derivative of $g$ at point $x$. \n",
    "Applying this method to the optimizaton problem $ \\min_{x \\in \\mathbb{R}^n} f(x)$ consists in setting $g(x) = \\nabla f(x)$, i.e. looking for stationnary points.  The iterations read: \n",
    "$$x_{k+1}  = x_k - \\nabla^2 f(x_k)^{-1} \\nabla f(x_k)  .$$\n",
    "\n",
    "A quasi-Newton method reads :\n",
    "$$x_{k+1}  = x_k - p_k * B_k \\nabla f(x_k) .$$\n",
    "where $B_k$ is a matrix which aims to approximate the inverse of the Hessian of $f$ at $x_k$. The BFGS algorithms uses a correction formula of rank 2 in order to approximate the Hessian (more details are available [here](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)).\n",
    "\n",
    "\n",
    "\n",
    "The [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) is an adaptation of the BFGS method for large-scale problems. While BFGS requires to store in memory a matrix of the size of the Hessian, $d * d$  (with $d$ the number of variables), which can be prohibitive in applications such as computer vision or machine learning, the L-BFGS algorithm only store a few vectors that are used to approximate the hessian. As a consequence, the memory usage is linear in the dimension of the problem.\n",
    "\n",
    "\n",
    "Nowadays, there is an increasing demand to deal with massive instances and variables. In such a context, it is important to scale up and parallelize the L-BFGS algorithm in a distributed system. In this project, we implement *(i)* the L-BFGS algorithm and *(ii)* the **Vector-free L-BFGS** (VL-BFGS) using MapReduce and Pyspark. The VL-BFGS, developed by Weizhu Chen, Zhenghao Wang and Jingren Zhou (Microsoft - [\"*Large-scale L-BFGS using MapReduce*\"](https://ai2-s2-pdfs.s3.amazonaws.com/2e07/77d0cd13f31f0abae97e824111c04e330f40.pdf) ), is a modification of the L-BFGS algorithm to avoid the expensive dot product operations in the L-BFGS implementation. The methods are illustrated using a logistic function with a ridge penalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fct'></a>\n",
    "## 2. The logistic function\n",
    "\n",
    "We want to minimize\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n \\ell(a_i^\\top x, b_i) + \\frac \\lambda 2 \\|x\\|_2^2\n",
    "$$\n",
    "where $\\ell(z, b) = \\log(1 + \\exp(-bz))$ (logistic regression), $n$ is the sample size, and labels $b_i \\in \\{ -1, 1 \\}$ for all $i$.\n",
    "\n",
    "We write it as a minimization problem of the form\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n f_i(x)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f_i(x) = \\ell(a_i^\\top x, b_i) + \\frac \\lambda 2 \\|x\\|_2^2.\n",
    "$$\n",
    "\n",
    "The gradient is\n",
    "$$\n",
    "\\nabla f_i(x) = - \\frac{b_i}{1 + \\exp(b_i a_i^\\top x)} a_i + \\lambda x.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce a class that will be used for finding the minimum of the function with scipy implementation of L-BFGS  (i.e. without MapReduce)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "class LogReg(object):\n",
    "    \"\"\"A class for the logistic regression with L2/Ridge penalization\"\"\"\n",
    "\n",
    "    def __init__(self, A, b, lbda):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.n, self.d = A.shape\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    def grad(self, x):\n",
    "        bAx = self.b * np.dot(self.A, x)\n",
    "        temp = 1. / (1. + np.exp(bAx))\n",
    "        grad = - np.dot(self.A.T, self.b * temp) / self.n + self.lbda * x\n",
    "        return grad\n",
    "\n",
    "    def loss(self, x):\n",
    "        bAx = self.b * np.dot(self.A, x)\n",
    "        return np.mean(np.log(1. + np.exp(- bAx))) + self.lbda * norm(x) ** 2 / 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 3. Generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Coefficients of the model')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1defbf88f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Getting sparse coefficients\n",
    "d = 50\n",
    "n = 10000\n",
    "idx = np.arange(d)\n",
    "\n",
    "# Ground truth coefficients of the model\n",
    "x_model = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "# Reprezentation of the coefficients\n",
    "plt.stem(x_model); \n",
    "plt.title(\"Coefficients of the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a dataset for the logistic using the sparse coefficients above ($d = 50$ variables and $n = 10,000$ observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal, randn\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "\n",
    "def simu_logreg(x, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation for the logistic\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (d,) - The coefficients of the model\n",
    "    n : int - Sample size\n",
    "    std : float, default=1. - Standard-deviation of the noise\n",
    "    corr : float, default=0.5 - Correlation of the features matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : ndarray, shape (n, d) - The design matrix.\n",
    "    b : ndarray, shape (n,) - The targets.\n",
    "    \"\"\"\n",
    "    d = x.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, d))\n",
    "    A = multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    noise = std * randn(n)\n",
    "    b = A.dot(x) + noise\n",
    "    \n",
    "    return A, np.sign(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 50)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "A, b = simu_logreg(x_model, n, std=1., corr=0.1)\n",
    "print(A.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93443765,  1.53897766,  0.87883136,  0.32909696, -0.00312574,\n",
       "        1.19904064,  0.84376087, -0.57428624, -0.72609293,  1.01055425,\n",
       "        0.32244906, -0.38427615, -2.30057774, -2.11715015,  2.38524573,\n",
       "       -1.53472855, -0.64382362, -0.59032009,  0.4171411 , -0.19287553,\n",
       "       -0.68745019, -0.98440094, -0.83311087,  0.94571311,  0.2115075 ,\n",
       "        0.83600399,  0.21443663, -0.53038224, -0.52368819, -0.9246443 ,\n",
       "       -1.90857486, -1.63579176, -0.31101506,  0.31680811,  0.86527269,\n",
       "        1.22465279,  0.64764891, -0.17165961, -0.48033534, -1.06956696,\n",
       "        0.34459552, -0.27105369, -1.38155863, -1.12992918,  0.57070852,\n",
       "        0.80647872,  1.33773964, -0.05558402,  1.03337773,  0.63466663])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sc'></a>\n",
    "\n",
    "## 4. L-BFGS with scipy (without MapReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we check that the gradient and the loss function of the \"*class LogReg*\" are correct. We also compute the minimum of the logistic loss function using Scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1563612654123882e-07"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "lbda = 1. / n ** (0.5) # default value\n",
    "\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(LogReg(A, b, lbda).loss, LogReg(A, b, lbda).grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *\"grad\"* function seems to be correct: the check_grad output is very small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe min of the logistic loss function is:  \u001b[0m 0.362237760275\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "# Initialization\n",
    "x_init = [random.random() for i in range(d)] \n",
    "x_min, f_min, _ = fmin_l_bfgs_b(LogReg(A, b, lbda).loss, x_init, LogReg(A, b, lbda).grad, pgtol=1e-30, factr=1e-30)\n",
    "print(\"\\033[1mThe min of the logistic loss function is:  \\033[0m\", f_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='algo1'></a>\n",
    "\n",
    "## 5. L-BFGS with MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the L-BFGS algorithm with MapReduce using PySpark. We reproduce *\"Algorithm 1\"* and *\"Algorithm 2\"* of the paper [\"*Large-scale L-BFGS using MapReduce*\"](https://ai2-s2-pdfs.s3.amazonaws.com/2e07/77d0cd13f31f0abae97e824111c04e330f40.pdf). \n",
    "\n",
    "First of all, we create a Resilient Distributed Dataset (RDD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 51)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((np.array(b,ndmin=2).T,A), axis = 1)\n",
    "data = sc.parallelize(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -0.06645776,  1.27286674, -1.48701828, -0.23326064,\n",
       "       -1.10796668,  1.23938816,  0.78096155,  0.43858165, -0.01083245,\n",
       "       -0.19418369, -0.07592908, -0.51612939,  0.1111656 , -0.48210349,\n",
       "        0.5902311 , -1.44742222, -0.77450085,  0.27834112,  0.85879126,\n",
       "        0.16048755, -0.40092992,  1.77957102, -0.54408148, -0.5343709 ,\n",
       "        1.62275196, -0.56146455,  0.31254364, -0.53210514, -0.97075471,\n",
       "        0.60499511, -1.19801148, -1.86047898,  1.76080196, -0.8638364 ,\n",
       "       -0.22273859, -1.84777344, -0.06935159,  0.73609321, -0.07875576,\n",
       "       -0.88530925,  2.12849386, -0.15291241,  0.05251885, -0.81073761,\n",
       "       -1.97074372, -0.21208668,  0.03700692,  0.6761332 , -0.27204531,\n",
       "       -1.0154034 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First entry\n",
    "data.first()\n",
    "# All the entries\n",
    "#data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map $\\&$ Reduce methods:**\n",
    "\n",
    "The map step takes each observation (each unit that can be treated independently) and applies some transformation to it. The reduction step then takes the results and does some sort of aggregation operation that returns a summary measure.\n",
    "\n",
    "\n",
    "The computation of the gradient can be written as some iterations over a Map step and a Reduce step. More precisely, we can compute \"partial\" gradients on each row of the sample (*Map*) and we can aggregate these \"partial\" gradients to obtain a gloabal gradient vector (*Reduce*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_rdd(matrix, x):\n",
    "    \"\"\"Return partial gradients that will be used later for computing the total gradient with MapReduce\"\"\"\n",
    "    Y = matrix[0]\n",
    "    X = matrix[1:]\n",
    "    return ((1.0 / (1.0 + np.exp(-Y * X.dot(x))) - 1.0) * Y * X.T + [lbda * x[i] for i in range(len(x))] )\n",
    "\n",
    "def add(x,y):\n",
    "    x +=y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_bfgs(data, gradient_rdd, max_iter, step, x_init):\n",
    "    \"\"\"\n",
    "    L-BFGS algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : RDD \n",
    "    gradient_rdd : function computing partial gradients (see above)\n",
    "    max_iter : number of iterations\n",
    "    step_value : step used to update the value of x\n",
    "    x_init : ndarray, shape (d,) - The initialization of the coefficients of the model\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : ndarray, shape (d,) - The argmin of the loss function.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\033[1mRunning L-BFGS with %s iterations: \\033[0m\" % max_iter)\n",
    "    print(\"The step used is %s\" % step)\n",
    "    \n",
    "    # Initialization\n",
    "    x = x_init\n",
    "    d = len(x_init)\n",
    "    m1 = 10 # m1 is the integer history size\n",
    "    k = 0\n",
    "    alpha = [1]\n",
    "    \n",
    "    # Gradient using MapReduce\n",
    "    res = data.map(lambda m: gradient_rdd(m,x)).reduce(add)\n",
    "    gradient_val = res\n",
    "   \n",
    "    s = np.array(x, ndmin=2).T # position difference initialization\n",
    "    y = np.array(gradient_val, ndmin=2).T # gradient difference initialization \n",
    "    \n",
    "    \n",
    "    for nb_iter in range(0, max_iter):   \n",
    "        # Search direction (parameter pk) (algorithm 2 in the paper)\n",
    "        p_k = list(-gradient_val)\n",
    "        for i in range(s.shape[1]):\n",
    "            alpha[i]=s[:,i].T.dot(np.array(p_k,ndmin=2).T[:,0])/s[:,i].T.dot(y[:,i])\n",
    "            p_k= [round(p_k[j]-alpha[i]*y[j,i],3) for j in range(d)]\n",
    "        if k > 0:\n",
    "            coeff=(s[:,0].T.dot(y[:,0])/y[:,0].T.dot(y[:,0]))\n",
    "            p_k=[round(coeff*p_k[m],3) for m in range(len(p_k))]\n",
    "        for j in range(s.shape[1]):\n",
    "            b=y[:,k-1-j].T.dot(p_k)/s[:,k-1-j].T.dot(y[:,k-1-j])\n",
    "            p_k=[round(p_k[m]+(alpha[k-1-j]-b)*s[m,k-1-j],3) for m in range(d)]\n",
    "           \n",
    "        # Compute the new value of x\n",
    "        x = [x[m] + step * p_k[m] for m in range(d)]\n",
    "         \n",
    "        # Discard vector pairs s_k-m, y_k-m from memory storage\n",
    "        if k > m1:\n",
    "            s=np.delete(s,s.shape[1]-1,1)\n",
    "            y=np.delete(y,y.shape[1]-1,1)\n",
    "        else:\n",
    "            k=k+1\n",
    "        \n",
    "        # Convergence check\n",
    "        l_inf_norm_grad = np.max(np.abs(gradient_val))\n",
    "        #print('iter: %d, l_inf_norm(grad): %.6g, f_min: %.6g'  %(nb_iter, l_inf_norm_grad, LogReg(A, b, lbda).loss(x)))\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break \n",
    "            \n",
    "        # Compute gradient for n_iter + 1\n",
    "        res = data.map(lambda m: gradient_rdd(m, x)).reduce(add)\n",
    "        gradient1 = res\n",
    "        \n",
    "        # Update the position and gradient differences\n",
    "        alpha+=[1]\n",
    "        s = np.concatenate((np.array([step * p_k[m] for m in range(d)], ndmin=2).T,s), axis=1)\n",
    "        y = np.concatenate((np.array((gradient1 - gradient_val), ndmin=2).T,y), axis=1)\n",
    "        gradient_val = gradient1\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRunning L-BFGS with 50 iterations: \u001b[0m\n",
      "The step used is 0.33\n",
      "The running time is 144.9526400566101 \n",
      "The minimum is 0.363747574983 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "x_min_algo1 = L_bfgs(data, gradient_rdd, max_iter=50, step=0.33, x_init=x_init)\n",
    "time1 = time.time() - start\n",
    "\n",
    "print(\"The running time is %s \" % time1)\n",
    "f_min_algo1 = LogReg(A, b, lbda).loss(x_min_algo1)\n",
    "print(\"The minimum is %s \" % f_min_algo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to minimum -0.00150981470759\n"
     ]
    }
   ],
   "source": [
    "print(\"Distance to minimum\", f_min - f_min_algo1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "Our implementation gives a minimum that is very close to the \"scipy\" minimum. The running time is about 144.95 seconds (2.4 minutes).\n",
    "In addition, L-BFGS has the advantage to maintain the historical states of previous $m1$ (we choose $m1=10$ in our algorithm) updates of current position $x$ and its gradient. The BFGS algorithm would have stored a dense $d*d$ ($d = 50$ in our case) matrix instead.\n",
    "\n",
    "\n",
    "The two-loop recursion in the algorithm are used to compute the direction $p_k$ : it initalizes the direction $p_k$ with gradient and continues to update it using historical states $y$ and $s$. It requires $2m1 +1$ (i.e. 21) vectors and each of them has a length of $d$ (i.e. 50). \n",
    "When $d$ is in billion scale, either the storage or the computation cost becomes prohibitive and it makes it impractical to implement in MapReduce.\n",
    "\n",
    "We need an algorithm that limits both the memory consumption and the number of MapReduce steps per iteration.\n",
    "\n",
    "\n",
    "\n",
    "*Note: we could have computed a step size using a line search to satisfy the strong Wolfe conditions but we were satisfied with our default step size (0.33).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='algo2'></a>\n",
    "\n",
    "## 6. VL-BFGS with MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the VL-BFGS algorithm.\n",
    "\n",
    "The intuition behind the VL-BFGS algorithm is that $p_k$ can be represented as a linear combination of $y$, $s$ and $\\nabla f(x)$. Let's define $(2m1+1)$ base vectors: \n",
    "- $b_1 = s_{k-m1}, b_2 = s_{k-m1+1},...., b_{m1} = s_{k-1}$\n",
    "- $b_{m1+1} = y_{k-m},...b_{2m1} = y_{k-1}$\n",
    "- $b_{2m1+1} = \\nabla f(x_i)$.\n",
    "\n",
    "$p$ is formalized as a linear combination of $b_i$ : $ p = \\sum_{k=1}^{2m1+1} \\delta_k b_k $   [1].\n",
    "\n",
    "\n",
    "The dot product operations used for computing $p_k$ in the algorithm L-BFGS can be classified into two categories:\n",
    "- the dot product between $(s_i, y_i)$ - we can pre-compute their dot product;\n",
    "- the dot product involving $p$ - since the direction $p$ is changing during the for loop, the dot product cannot be pre computed. However, thanks to the linear decompositiohn of $p$ in [1], we can decompose any dot product involving $p_k$ into a summation of dot products with its based vectors and corresponding coefficients.\n",
    "\n",
    "The VL-BFGS algorithm uses the dot products between every two base vectors as a scalar matrix of $(2m1 +1) *(2m1+1)$ scalars for computing the direction $p_k$. As the original L-BFGS algorithm, it has a two loop recursion but all the operations are only dependent on scalar operations (more details [here](https://ai2-s2-pdfs.s3.amazonaws.com/2e07/77d0cd13f31f0abae97e824111c04e330f40.pdf)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VL_bfgs(data, gradient_rdd, max_iter, step, x_init):\n",
    "    \"\"\"\n",
    "    VL-BFGS algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : RDD \n",
    "    gradient_rdd : function computing partial gradients (see above)\n",
    "    max_iter : number of iterations\n",
    "    step_value : step used to update the value of x\n",
    "    x_init : ndarray, shape (d,) - The initialization of the coefficients of the model\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : ndarray, shape (d,) - The argmin of the loss function.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\033[1mRunning VL-BFGS with %s iterations: \\033[0m\" % max_iter)\n",
    "    print(\"The step used is %s\" % step)\n",
    "    \n",
    "    # Initialization\n",
    "    x = x_init\n",
    "    d = len(x_init)\n",
    "    \n",
    "    m1 = 10 # m1 is the integer history size\n",
    "    k = 1 \n",
    "    alpha = [1]\n",
    "    \n",
    "    # Gradient using MapReduce\n",
    "    res = data.map(lambda m: gradient_rdd(m,x)).reduce(add)\n",
    "    gradient_val = res\n",
    "   \n",
    "    s = np.array(x, ndmin=2).T # position difference initialization\n",
    "    y = np.array(gradient_val, ndmin=2).T # gradient difference initialization \n",
    "    \n",
    "    matriceglobale1 = np.concatenate((np.array(gradient_val,ndmin=2).T,y,s),axis=1)\n",
    "    matriceglobale1 = matriceglobale1[:,::-1]\n",
    "    matriceglobale = matriceglobale1.T @ matriceglobale1\n",
    "\n",
    "    \n",
    "    for nb_iter in range(0, max_iter): \n",
    "        \n",
    "        if k >= m1:\n",
    "            # Search direction (parameter pk) - algorithm 3 of the paper\n",
    "            delta = [0 if i<= 2*m1-1 else -1 for i in range(2*m1+1)]\n",
    "            for i in reversed(range(k-m1,k)):\n",
    "                j = i - (k-m1) + 1\n",
    "                alpha[i] = sum([delta[l] * matriceglobale[l,j] for l in range(2*m1 + 1)]) / matriceglobale[j, m1+j]\n",
    "                delta[m1+j] = delta[m1+j] - alpha[i]\n",
    "            for i in range(2*m1+1):\n",
    "                delta[i] = matriceglobale[m1,2*m1] * delta[i] / matriceglobale[2*m1, 2*m1]\n",
    "            for i in range(k-m1,k):\n",
    "                j = i-(k-m1)+1\n",
    "                beta = sum([delta[l]*matriceglobale[m1+j,l] for l in range(2*m1+1)])/matriceglobale[j,m1+j]\n",
    "                delta[j] = delta[j] + (alpha[i]-beta)        \n",
    "            p_k = matriceglobale1.dot(np.array(delta, ndmin= 2).T)\n",
    "            p_k = p_k / max(np.abs(p_k))\n",
    "          \n",
    "            # Compute the new value of x\n",
    "            x = [x[m] + step * p_k[m,0] for m in range(d)]   # step = 0.33 bien\n",
    "        \n",
    "            # Discard vector pairs s_k-m, y_k-m from memory storage\n",
    "            s=np.delete(s,s.shape[1]-1,1)\n",
    "            y=np.delete(y,y.shape[1]-1,1)\n",
    "  \n",
    "        \n",
    "        else : \n",
    "            # Search direction (parameter pk) - algorithm 3 of the paper\n",
    "            delta = [0 if i<=2*k-1 else -1 for i in range(2*k+1)]\n",
    "            for i in reversed(range(k)):\n",
    "                j = i +1\n",
    "                alpha[i] = sum([delta[l] * matriceglobale[l,j] for l in range(2*k+1)]) / matriceglobale[j,k+j]\n",
    "                delta[k+j] = delta[k+j]-alpha[i]\n",
    "            for i in range(2*k+1):\n",
    "                delta[i] = matriceglobale[k,2*k] * delta[i]/matriceglobale[2*k,2*k]\n",
    "            for i in range(k):\n",
    "                j = i+1\n",
    "                beta = sum([delta[l] * matriceglobale[k+j,l] for l in range(2*k+1)]) / matriceglobale[j,k+j]\n",
    "                delta[j] = delta[j] + (alpha[i]-beta)\n",
    "            p_k = matriceglobale1.dot(np.array(delta, ndmin= 2).T)\n",
    "            p_k = p_k / max(np.abs(p_k))\n",
    "           \n",
    "            # Compute the new value of x\n",
    "            x = [x[m] + step * p_k[m,0] for m in range(d)]   # step = 0.33 bien\n",
    "            \n",
    "        # Convergence check\n",
    "        l_inf_norm_grad = np.max(np.abs(gradient_val))\n",
    "        #print('iter: %d, l_inf_norm(grad): %.6g, f_min: %.6g'  %(nb_iter, l_inf_norm_grad, LogReg(A, b, lbda).loss(x)))\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break \n",
    "            \n",
    "        # Compute gradient for n_iter + 1\n",
    "        res = data.map(lambda m: gradient_rdd(m, x)).reduce(add)\n",
    "        gradient1 = res\n",
    "        \n",
    "        # Update the position and gradient differences\n",
    "        alpha += [1]\n",
    "        s = np.concatenate((step*p_k,s), axis=1)\n",
    "        y = np.concatenate((np.array((gradient1 - gradient_val), ndmin=2).T,y), axis=1)\n",
    "        gradient_val = gradient1\n",
    "        matriceglobale1=np.concatenate((np.array(gradient_val,ndmin=2).T,y,s),axis=1)\n",
    "        matriceglobale1=matriceglobale1[:,::-1]\n",
    "        matriceglobale = matriceglobale1.T @ matriceglobale1\n",
    "        k=k+1\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRunning VL-BFGS with 50 iterations: \u001b[0m\n",
      "The step used is 0.33\n",
      "The running time is 136.98225808143616 \n",
      "The minimum is 0.762606504308 \n",
      "Distance to minimum is -0.400368744033\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "x_min_algo2 = VL_bfgs(data, gradient_rdd, max_iter=50, step=0.33, x_init=x_init)\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(\"The running time is %s \" % time2)\n",
    "f_min_algo2 = LogReg(A, b, lbda).loss(x_min_algo2)\n",
    "print(\"The minimum is %s \" % f_min_algo2)\n",
    "print(\"Distance to minimum is\", f_min - f_min_algo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "The L-BFGS and VL-BFGS algorithms are mathematically equivalent. However, since all calculations are based on salars, the VL-BFGS is more efficient. It requires only $8m1^2$ multiplications between scalar in the two for loops.\n",
    "The running time of the algorithm is a bit smaller and equal to 136.98 seconds (2.3 minutes). However, the algorithm needs more iterations to converge to the minimum (50 is not enough) (or maybe we did something wrongly -- the way for obtaining the matrix dot product *\"matriceglobale1\"* was not detailed in the paper).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach is to store $s$ and $y$ into multiple partitions and use a MapReduce step to calculate every dot product. We tried this approach but the *\"Reduce\"* step was very expensive in terms of running time (see below). It is the reason why we used the \"@\" operator for the calculation of the dot product matrix.\n",
    "We give an example hereafter with a $3 \\times 3$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our initial (?)bad(?) idea: parallelization of the product matrix\n",
    "\n",
    "import pandas as pd\n",
    "def add(x,y):\n",
    "    x +=y\n",
    "    return x\n",
    "\n",
    "def process_mat_row(row):\n",
    "    index = int(row[0])\n",
    "    values = [float(_) for _ in row[1:]]\n",
    "    return [[index, j, v] for j, v in enumerate(values)]\n",
    "\n",
    "def key_ij(row):\n",
    "    return row[0], (row[1], row[2])\n",
    "def key_ji(row):\n",
    "    return row[1], (row[0], row[2])\n",
    "\n",
    "def produit_matriciel(row):\n",
    "    index, ((i, v1), (j, v2)) = row\n",
    "    return i, j, v1 * v2\n",
    "\n",
    "def produit(mat1, mat2):\n",
    "        matrice1 = mat1.flatMap(process_mat_row)\n",
    "        matrice2 = mat2.flatMap(process_mat_row)\n",
    "        mat_join = matrice1.map(key_ji).join(matrice2.map(key_ij))\n",
    "        produit = mat_join.map(produit_matriciel)\n",
    "        final = produit.map(lambda row:((row[1],row[0]), row[2])).reduceByKey(add).reduce(add)\n",
    "        return(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [1 4 5 6]\n",
      " [2 2 3 4]]\n",
      "[[0 1 4 2]\n",
      " [1 2 5 3]\n",
      " [2 3 6 4]]\n",
      "Matrix product\n",
      "[[ 14.  32.  20.]\n",
      " [ 32.  77.  47.]\n",
      " [ 20.  47.  29.]]\n",
      "Running time for a 3*3 matrix :  12.800367832183838\n"
     ]
    }
   ],
   "source": [
    "# Test with 3*3 Matrix\n",
    "mat1 = np.array([[0,1,2,3],[1,4,5,6],[2,2,3,4]])\n",
    "mat2 = np.array([[0,1,4,2],[1,2,5,3],[2,3,6,4]])\n",
    "print(mat1)\n",
    "print(mat2)\n",
    "mat1 = sc.parallelize(mat1)\n",
    "mat2 = sc.parallelize(mat2)\n",
    "\n",
    "start = time.time()\n",
    "u = produit(mat1,mat2)\n",
    "# Sort the elements\n",
    "u1 = np.zeros((3,3))\n",
    "for i in range(int(len(u)/2)):\n",
    "    u1[u[2*i][0],u[2*i][1]]=u[2*i+1]\n",
    "time_need = time.time() - start\n",
    "print(\"Matrix product\")\n",
    "print(u1)\n",
    "print(\"Running time for a 3*3 matrix : \", time_need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conc'></a>\n",
    "## 7. Conclusion\n",
    "\n",
    "**Comparison of the L-BFGS and VL-BFGS algorithms (50 iterations, 50 variables, 10,000 observations):**\n",
    "\n",
    "\n",
    "| Algorithm     | F_min         | Running time |\n",
    "| ------------- |:-------------:| ------------:|\n",
    "| Scipy         | 0.36          |      -       |\n",
    "| L-BFGS        | 0.36          |   144.95     |\n",
    "| VL-BFGS       | 0.76          |   136.98     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude the VL-BFGS algorithm enables a variety of machine learning algorithms to handle a massive number of variables over large datasets. \n",
    "\n",
    "The better scalability property of the VL-BFGS seems negligible with our implementation because we took a very small number of variables (50) and we ran MapReduce locally. The autors of the [paper](https://ai2-s2-pdfs.s3.amazonaws.com/2e07/77d0cd13f31f0abae97e824111c04e330f40.pdf) have shown a huge performance improvement with the VL-BFGS using 1,038,934,683 features and running the experiment in a shared cluster with tens of thousands of machines (each of the machine had up to 12 concurrent vertices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we consider as use-case a logistic regression with ridge penalization only, although the algorithms above can be used with many other models, and other types of penalization, eventually non-smooth ones, such as the $\\ell_1$ penalization (requires to use the proximal operator instead of the gradient).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
